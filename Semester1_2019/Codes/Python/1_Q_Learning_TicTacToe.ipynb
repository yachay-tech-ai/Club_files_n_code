{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Tic Tac Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, uniform\n",
    "from sklearn.externals import joblib\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self, player1=\"X\", player2=\"O\"):\n",
    "        self.player1 = player1\n",
    "        self.player2 = player2\n",
    "        self.board = None\n",
    "        self.actions = [(0,0),(0,1),(0,2),(1,0),(1,1),(1,2),(2,0),(2,1),(2,2)]\n",
    "    \n",
    "    def initGame(self):\n",
    "        self.board = []\n",
    "        for i in range(3):\n",
    "            row = []\n",
    "            for j in range(3):\n",
    "                row.append(' ')\n",
    "            self.board.append(row)\n",
    "    \n",
    "    def printBoard(self):\n",
    "        clear_output()\n",
    "        empty_board = \"      ___ ___ ___\\n     |   |   |   |\\n     | 0 | 1 | 2 |\\n  ___|___|___|___|\\n |   |   |   |   |\\n | 0 |   |   |   |\\n |___|___|___|___|\\n |   |   |   |   |\\n | 1 |   |   |   |\\n |___|___|___|___|\\n |   |   |   |   |\\n | 2 |   |   |   |\\n |___|___|___|___|\\n\"\n",
    "        '''   ___ ___ ___\n",
    "             |   |   |   |\n",
    "             | 0 | 1 | 2 |\n",
    "          ___|___|___|___|\n",
    "         |   |   |   |   |\n",
    "         | 0 | a | b | c |\n",
    "         |___|___|___|___|\n",
    "         |   |   |   |   |\n",
    "         | 1 | d | e | f |\n",
    "         |___|___|___|___|\n",
    "         |   |   |   |   |\n",
    "         | 2 | g | h | i |\n",
    "         |___|___|___|___|'''\n",
    "        a = self.board[0][0] \n",
    "        b = self.board[0][1] \n",
    "        c = self.board[0][2] \n",
    "        d = self.board[1][0] \n",
    "        e = self.board[1][1] \n",
    "        f = self.board[1][2] \n",
    "        g = self.board[2][0] \n",
    "        h = self.board[2][1] \n",
    "        i = self.board[2][2] \n",
    "        current_board = empty_board[0:101] + a + empty_board[102:105] + b + empty_board[106:109] + c + empty_board[110:158] + d + empty_board[159:162] + e + empty_board[163:166] + f + empty_board[167:215] + g + empty_board[216:219] + h + empty_board[220:223] + i + empty_board[224:]\n",
    "        print(current_board)\n",
    "        print(\"-\"*30)\n",
    "\n",
    "    def checkWin(self, player):\n",
    "        mark = 'X' if player==1 else 'O'\n",
    "        for i in range(3):\n",
    "            if (self.board[i][0]==mark and self.board[i][1]==mark and self.board[i][2]==mark):\n",
    "                return True\n",
    "            if (self.board[0][i]==mark and self.board[1][i]==mark and self.board[2][i]==mark):\n",
    "                return True\n",
    "        if (self.board[0][0] == mark and self.board[1][1]==mark and self.board[2][2]==mark):\n",
    "            return True\n",
    "        if (self.board[2][0] == mark and self.board[1][1]==mark and self.board[0][2]==mark):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def checkEqual(self):\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if self.board[i][j] == ' ':\n",
    "                    return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def getPossibleActions(self):\n",
    "        possible_actions = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if self.board[i][j] == ' ':\n",
    "                    possible_actions .append((i,j))\n",
    "        return possible_actions \n",
    "\n",
    "    def getAction(self):\n",
    "        possible_actions = self.getPossibleActions()\n",
    "        action = possible_actions[randint(0,len(possible_actions)-1)]\n",
    "        return action\n",
    "\n",
    "    def calculateReward(self, player, player_training):\n",
    "        if self.checkWin(player):\n",
    "            if player_training == 1:\n",
    "                if player == 1:\n",
    "                    return 20, True\n",
    "                else:\n",
    "                    return -10, True\n",
    "            else:\n",
    "                if player == 2:\n",
    "                    return 20, True\n",
    "                else:\n",
    "                    return -10, True\n",
    "        elif self.checkEqual():\n",
    "            return -10, True\n",
    "        else:\n",
    "            return -1, False\n",
    "    \n",
    "    def play(self, action, player, player_training):\n",
    "        self.board[action[0]][action[1]] = 'X' if player == 1 else 'O'\n",
    "        return self.calculateReward(player, player_training)\n",
    "\n",
    "    def playRandom(self, player, player_training):\n",
    "        action = self.getAction()\n",
    "        self.board[action[0]][action[1]] = 'X' if player == 1 else 'O'\n",
    "        return self.calculateReward(player, player_training)\n",
    "\n",
    "    def verifyAction(self, action):\n",
    "        possible_actions = self.getPossibleActions()\n",
    "        if action in possible_actions:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def playHuman(self, player):\n",
    "        while True:\n",
    "            action = input(\"Input the action: \")\n",
    "            try:\n",
    "                action = action.split(\",\")\n",
    "            except:\n",
    "                print(\"The action needs to be two numbers between 0-2. Ex: 0,1. Try again\")\n",
    "            try:    \n",
    "                action[0] = int(action[0])\n",
    "                action[1] = int(action[1])\n",
    "                action = tuple(action)\n",
    "                if self.verifyAction(action):\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"The action needs to be two numbers between 0-2. Ex: 0,1. Try again\")\n",
    "            except:\n",
    "                print(\"The action needs to be two numbers between 0-2. Ex: 0,1. Try again\")\n",
    "        return self.play(action, player, 1)\n",
    "    \n",
    "    def initRealGame(self):\n",
    "        self.player1 = \"Q-Learning Agent\"\n",
    "        self.player2 = input(\"Input the name of the player: \")\n",
    "        q_table_player1 = joblib.load(\"q_table_player1.sav\")\n",
    "        states_player1 = joblib.load(\"states_player1.sav\")\n",
    "        self.initGame()\n",
    "        self.printBoard()\n",
    "        _ = input(f\"Press enter to start the game and good luck {self.player2} ;)\")\n",
    "        while True:\n",
    "            possible_actions = self.getPossibleActions()\n",
    "            index_state = states_player1.index(self.board)\n",
    "            probabilities_possible_actions = []\n",
    "            indexes_possible_actions = []\n",
    "            for action in possible_actions:\n",
    "                action_index = self.actions.index(action)\n",
    "                probabilities_possible_actions.append(q_table_player1[index_state][action_index])\n",
    "                indexes_possible_actions.append(action_index)\n",
    "            max_prob = max(probabilities_possible_actions)\n",
    "            index_max_prob = indexes_possible_actions[probabilities_possible_actions.index(max_prob)]\n",
    "            action = self.actions[index_max_prob]\n",
    "        \n",
    "            reward_player1, end_game = self.play(action, 1, 1)  \n",
    "            self.printBoard()\n",
    "            if end_game:\n",
    "                if reward_player1 == 20:\n",
    "                    print(f\"{env.player1} wins!\")\n",
    "                if reward_player1 == -10:\n",
    "                    print(\"Tie\")\n",
    "                break\n",
    "            \n",
    "            _ , end_game = self.playHuman(2)\n",
    "            if end_game:\n",
    "                print(f\"{env.player2} wins!\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "threshold = 0.1\n",
    "env = TicTacToe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training player 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_player1 = []\n",
    "q_table_player1 = []\n",
    "for i in range(1,1000001):\n",
    "    env.initGame()\n",
    "    while True:\n",
    "        if env.board not in states_player1:\n",
    "            states_player1.append(deepcopy(env.board))\n",
    "            q_table_player1.append([0,0,0,0,0,0,0,0,0])\n",
    "            action = env.getAction() \n",
    "            max_prob = 0\n",
    "            index_state = -1\n",
    "            index_max_prob = env.actions.index(action)\n",
    "        else:   \n",
    "            index_state = states_player1.index(env.board)\n",
    "            if uniform(0, 1) < threshold:\n",
    "                action = env.getAction()\n",
    "                index_max_prob = env.actions.index(action)\n",
    "                max_prob = q_table_player1[index_state][index_max_prob]\n",
    "            else:\n",
    "                possible_actions = env.getPossibleActions()\n",
    "                probabilities_possible_actions = []\n",
    "                indexes_possible_actions = []\n",
    "                for action in possible_actions:\n",
    "                    action_index = env.actions.index(action)\n",
    "                    probabilities_possible_actions.append(q_table_player1[index_state][action_index])\n",
    "                    indexes_possible_actions.append(action_index)\n",
    "                max_prob = max(probabilities_possible_actions)\n",
    "                index_max_prob = indexes_possible_actions[probabilities_possible_actions.index(max_prob)]\n",
    "                action = env.actions[index_max_prob]\n",
    "\n",
    "        reward_player1, end_game = env.play(action, 1, 1)  \n",
    "                \n",
    "        if end_game:   \n",
    "            old_value = max_prob\n",
    "            next_max = 0\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward_player1 + gamma * next_max)\n",
    "            q_table_player1[index_state][index_max_prob] = new_value\n",
    "            break\n",
    "        \n",
    "        reward_player2, end_game = env.playRandom(2, 1)\n",
    "        \n",
    "        if env.board not in states_player1:\n",
    "            states_player1.append(deepcopy(env.board))\n",
    "            q_table_player1.append([0,0,0,0,0,0,0,0,0])\n",
    "                \n",
    "        total_reward = reward_player1 + reward_player2\n",
    "        old_value = max_prob\n",
    "        index_next_state = states_player1.index(env.board)\n",
    "        next_max = max(q_table_player1[index_next_state])\n",
    "        new_value = (1 - alpha) * old_value + alpha * (total_reward + gamma * next_max)\n",
    "        q_table_player1[index_state][index_max_prob] = new_value\n",
    "\n",
    "        if end_game:\n",
    "            break\n",
    "\n",
    "    if i % 100000 == 0:\n",
    "        print(f\"Episode: {i}\")\n",
    "print(\"Training finished.\\n\")\n",
    "joblib.dump(q_table_player1, 'q_table_player1.sav')\n",
    "joblib.dump(states_player1, 'states_player1.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training player 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_player2 = []\n",
    "q_table_player2 = []\n",
    "for i in range(1,1000001):\n",
    "    env.initGame()\n",
    "    reward_player1, end_game = env.playRandom(1, 2)\n",
    "    if env.board not in states_player2:\n",
    "        states_player2.append(deepcopy(env.board))\n",
    "        q_table_player2.append([0,0,0,0,0,0,0,0,0])\n",
    "    while True:\n",
    "        index_state = states_player2.index(env.board)\n",
    "        if uniform(0, 1) < threshold:\n",
    "            action = env.getAction()\n",
    "            index_max_prob = env.actions.index(action)\n",
    "            max_prob = q_table_player2[index_state][index_max_prob]\n",
    "        else:\n",
    "            possible_actions = env.getPossibleActions()\n",
    "            probabilities_possible_actions = []\n",
    "            indexes_possible_actions = []\n",
    "            for action in possible_actions:\n",
    "                action_index = env.actions.index(action)\n",
    "                probabilities_possible_actions.append(q_table_player2[index_state][action_index])\n",
    "                indexes_possible_actions.append(action_index)\n",
    "            max_prob = max(probabilities_possible_actions)\n",
    "            index_max_prob = indexes_possible_actions[probabilities_possible_actions.index(max_prob)]\n",
    "            action = env.actions[index_max_prob]\n",
    "           \n",
    "        reward_player2, end_game = env.play(action, 2, 2)\n",
    "        if end_game:\n",
    "            old_value = max_prob\n",
    "            next_max = 0\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward_player2 + gamma * next_max)\n",
    "            q_table_player2[index_state][index_max_prob] = new_value\n",
    "            break\n",
    "        \n",
    "        reward_player1, end_game = env.playRandom(1, 2)\n",
    "        \n",
    "        if env.board not in states_player2:  \n",
    "            states_player2.append(deepcopy(env.board))\n",
    "            q_table_player2.append([0,0,0,0,0,0,0,0,0])\n",
    "        \n",
    "        total_reward = reward_player1 + reward_player2\n",
    "        old_value = max_prob\n",
    "        index_next_state = states_player2.index(env.board)\n",
    "        next_max = max(q_table_player2[index_next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (total_reward + gamma * next_max)\n",
    "        q_table_player2[index_state][index_max_prob] = new_value\n",
    "\n",
    "        if end_game:\n",
    "            break\n",
    "        \n",
    "    if i % 100000 == 0:\n",
    "        print(f\"Episode: {i}\")\n",
    "print(\"Training finished.\\n\")\n",
    "joblib.dump(q_table_player2, 'q_table_player2.sav')\n",
    "joblib.dump(states_player2, 'states_player2.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the performance of a random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_wins = []\n",
    "total_ties = []\n",
    "total_losses = []\n",
    "q_table_player2 = joblib.load(\"q_table_player2.sav\")\n",
    "states_player2 = joblib.load(\"states_player2.sav\")\n",
    "threshold = 0.5\n",
    "for _ in range(1000):\n",
    "    wins, ties, losses = 0, 0, 0\n",
    "    games = 100\n",
    "    for _ in range(games):\n",
    "        env.initGame()\n",
    "        while True:\n",
    "            reward_player1, end_game = env.playRandom(1, 1) \n",
    "            if end_game:\n",
    "                if reward_player1 == 20:\n",
    "                    wins += 1\n",
    "                if reward_player1 == -10:\n",
    "                    ties += 1\n",
    "                break     \n",
    "            \n",
    "            if uniform(0, 1) < threshold:\n",
    "                action = env.getAction()\n",
    "            else:\n",
    "                possible_actions = env.getPossibleActions()\n",
    "                index_state = states_player2.index(env.board)\n",
    "                probabilities_possible_actions = []\n",
    "                indexes_possible_actions = []\n",
    "                for action in possible_actions:\n",
    "                    action_index = env.actions.index(action)\n",
    "                    probabilities_possible_actions.append(q_table_player2[index_state][action_index])\n",
    "                    indexes_possible_actions.append(action_index)\n",
    "                max_prob = max(probabilities_possible_actions)\n",
    "                index_max_prob = indexes_possible_actions[probabilities_possible_actions.index(max_prob)]\n",
    "                action = env.actions[index_max_prob]\n",
    "\n",
    "            reward_player2, end_game = env.play(action, 2, 1)  \n",
    "                    \n",
    "            if end_game:\n",
    "                losses += 1\n",
    "                break\n",
    "            \n",
    "    total_wins.append(wins)\n",
    "    total_ties.append(ties)\n",
    "    total_losses.append(losses)\n",
    "\n",
    "print(\"RESULTS RANDOM PLAYER\")\n",
    "print(f\"Percentage of wins: {np.mean(total_wins)}%\")\n",
    "print(f\"Pertentage of ties: {np.mean(total_ties)}%\")\n",
    "print(f\"Percentage of losses: {np.mean(total_losses)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the performance of a Q-Learning Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_wins = []\n",
    "total_ties = []\n",
    "total_losses = []\n",
    "q_table_player1 = joblib.load(\"q_table_player1.sav\")\n",
    "states_player1 = joblib.load(\"states_player1.sav\")\n",
    "q_table_player2 = joblib.load(\"q_table_player2.sav\")\n",
    "states_player2 = joblib.load(\"states_player2.sav\")\n",
    "threshold = 0.5\n",
    "for _ in range(1000): \n",
    "    wins, ties, losses = 0, 0, 0\n",
    "    games = 100\n",
    "    for _ in range(games):\n",
    "        env.initGame()\n",
    "        while True:\n",
    "            possible_actions = env.getPossibleActions()\n",
    "            index_state = states_player1.index(env.board)\n",
    "            probabilities_possible_actions = []\n",
    "            indexes_possible_actions = []\n",
    "            for action in possible_actions:\n",
    "                action_index = env.actions.index(action)\n",
    "                probabilities_possible_actions.append(q_table_player1[index_state][action_index])\n",
    "                indexes_possible_actions.append(action_index)\n",
    "            max_prob = max(probabilities_possible_actions)\n",
    "            index_max_prob = indexes_possible_actions[probabilities_possible_actions.index(max_prob)]\n",
    "            action = env.actions[index_max_prob]\n",
    "    \n",
    "            reward_player1, end_game = env.play(action, 1, 1)  \n",
    "    \n",
    "            if end_game:\n",
    "                if reward_player1 == 20:\n",
    "                    wins += 1\n",
    "                if reward_player1 == -10:\n",
    "                    ties += 1\n",
    "                break\n",
    "            \n",
    "            if uniform(0, 1) < threshold:\n",
    "                action = env.getAction()\n",
    "            else:\n",
    "                possible_actions = env.getPossibleActions()\n",
    "                index_state = states_player2.index(env.board)\n",
    "                probabilities_possible_actions = []\n",
    "                indexes_possible_actions = []\n",
    "                for action in possible_actions:\n",
    "                    action_index = env.actions.index(action)\n",
    "                    probabilities_possible_actions.append(q_table_player2[index_state][action_index])\n",
    "                    indexes_possible_actions.append(action_index)\n",
    "                max_prob = max(probabilities_possible_actions)\n",
    "                index_max_prob = indexes_possible_actions[probabilities_possible_actions.index(max_prob)]\n",
    "                action = env.actions[index_max_prob]\n",
    "            \n",
    "            reward_player2, end_game = env.play(action, 2, 1)\n",
    "            \n",
    "            if end_game:\n",
    "                losses += 1\n",
    "                break\n",
    "            \n",
    "    total_wins.append(wins)\n",
    "    total_ties.append(ties)\n",
    "    total_losses.append(losses)\n",
    "\n",
    "print(\"RESULTS Q-LEARNING PLAYER\")\n",
    "print(f\"Percentage of wins: {np.mean(total_wins)}%\")\n",
    "print(f\"Pertentage of ties: {np.mean(total_ties)}%\")\n",
    "print(f\"Percentage of losses: {np.mean(total_losses)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the results of Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table_player1 = joblib.load(\"q_table_player1.sav\")\n",
    "states_player1 = joblib.load(\"states_player1.sav\")\n",
    "q_table_player2 = joblib.load(\"q_table_player2.sav\")\n",
    "states_player2 = joblib.load(\"states_player2.sav\")\n",
    "threshold = 0.1\n",
    "env.initGame()\n",
    "env.printBoard()\n",
    "_ = input(\"Press enter to continue: \")\n",
    "while True:\n",
    "    possible_actions = env.getPossibleActions()\n",
    "    index_state = states_player1.index(env.board)\n",
    "    probabilities_possible_actions = []\n",
    "    indexes_possible_actions = []\n",
    "    for action in possible_actions:\n",
    "        action_index = env.actions.index(action)\n",
    "        probabilities_possible_actions.append(q_table_player1[index_state][action_index])\n",
    "        indexes_possible_actions.append(action_index)\n",
    "    max_prob = max(probabilities_possible_actions)\n",
    "    index_max_prob = indexes_possible_actions[probabilities_possible_actions.index(max_prob)]\n",
    "    action = env.actions[index_max_prob]\n",
    "\n",
    "    reward_player1, end_game = env.play(action, 1, 1)  \n",
    "    env.printBoard()\n",
    "    _ = input(\"Press enter to continue: \")\n",
    "    if end_game:\n",
    "        if reward_player1 == 20:\n",
    "            print(f\"{env.player1} wins!\")\n",
    "        if reward_player1 == -10:\n",
    "            print(\"Tie\")\n",
    "        break\n",
    "    \n",
    "    if uniform(0, 1) < threshold:\n",
    "        action = env.getAction()\n",
    "    else:\n",
    "        possible_actions = env.getPossibleActions()\n",
    "        index_state = states_player2.index(env.board)\n",
    "        probabilities_possible_actions = []\n",
    "        indexes_possible_actions = []\n",
    "        for action in possible_actions:\n",
    "            action_index = env.actions.index(action)\n",
    "            probabilities_possible_actions.append(q_table_player2[index_state][action_index])\n",
    "            indexes_possible_actions.append(action_index)\n",
    "        max_prob = max(probabilities_possible_actions)\n",
    "        index_max_prob = indexes_possible_actions[probabilities_possible_actions.index(max_prob)]\n",
    "        action = env.actions[index_max_prob]\n",
    "    \n",
    "    reward_player2, end_game = env.play(action, 2, 1)\n",
    "    env.printBoard()\n",
    "    _ = input(\"Press enter to continue: \")\n",
    "    \n",
    "    if end_game:\n",
    "        print(f\"{env.player2} wins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing a Real Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.initRealGame()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
